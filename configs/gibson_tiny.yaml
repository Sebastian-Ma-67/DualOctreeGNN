# --------------------------------------------------------
# Dual Octree Graph Networks
# Copyright (c) 2022 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Peng-Shuai Wang
# --------------------------------------------------------

SOLVER:
  gpu: 0,1,2,3
  run: train

  logdir: logs/gibson_tiny/gibson_tiny
  max_epoch: 700
  test_every_epoch: 10
  log_per_iter: 50 
  ckpt_num: 200 # 保留 100 个checkpoint，如果有新的checkpoint被保存，则旧的就动态删除掉

  # optimizer
  type: adamw
  weight_decay: 0.01  # default value of adamw
  lr: 0.001           # default value of adamw

  # learning rate
  lr_type: poly
  step_size: (200, 300) 


DATA:
  train:
    name: pointcloud
    point_scale: 1.0

    # octree building
    depth: 8 # 八叉树的深度
    # full_depth 应该是一个阈值，当depth深度小于3的时候，force the octree to be full，
    # full 的意思就是变成了均匀的体素网格，不再有大网格小网格之分，全部都是三级小网格
    full_depth: 3 
    node_dis: True
    split_label: True
    offset: 0.0

    # data loading
    location: data/gibson_tiny
    filelist: data/gibson_tiny/manifest_train
    batch_size: 1 # 由原来的16改为8，怕显卡内存hold不住
    # num_workers: 0

    point_sample_num: 200000

  test:
    name: pointcloud
    point_scale: 1.0

    # octree building
    depth: 8
    full_depth: 3
    node_dis: True
    split_label: True
    offset: 0.0

    # data loading
    location: data/gibson_tiny
    filelist: data/gibson_tiny/manifest_test
    batch_size: 1 # 由原来的4改为2
    # num_workers: 0

    point_sample_num: 200000


MODEL:
  name: graph_unet
  resblock_type: basic
  find_unused_parameters: True

  depth: 8 # 这个好像是GraphOUNet网络的深度，最深好像就到8，
  full_depth: 3
  depth_out: 8 # The output feature depth
  channel: 4
  nout: 4

LOSS:
  name: dfaust
  loss_type: possion_grad_loss
